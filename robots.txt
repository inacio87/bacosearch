# robots.txt para https://bacosearch.com
# Este arquivo instrui os rastreadores de motores de busca sobre quais partes do seu site
# eles podem ou não acessar. É crucial para o SEO e para gerenciar o "orçamento de rastreamento".

User-agent: *
# As regras abaixo se aplicam a todos os rastreadores.

# --- Diretórios e Arquivos Sensíveis/Privados ---
# Bloquear pastas que contêm código-fonte, configurações, arquivos privados ou dados de usuários.
# Isso impede que rastreadores acessem áreas não destinadas à indexação pública.
Disallow: /core/             # Pasta de lógica interna do sistema
Disallow: /includes/         # Pasta de includes internos
Disallow: /uploads/          # Pasta de uploads (proteção de dados sensíveis e arquivos de usuário)
Disallow: /admin/            # Área administrativa (login, painéis, etc.)
Disallow: /api/              # Endpoints de API (não devem ser indexados)
Disallow: /auth/             # Páginas de autenticação (login, registro, recuperação de senha)
Disallow: /process_/         # Scripts de processamento de formulários (e.g., /process_register.php)
Disallow: /temp/             # Pastas temporárias
Disallow: /cache/            # Pastas de cache

# Bloquear arquivos específicos sensíveis ou de configuração que NUNCA devem ser acessados
# Mesmo que não existam, bots maliciosos podem procurar por eles.
Disallow: /.env              # Arquivo de variáveis de ambiente
Disallow: /config.php        # Arquivo de configuração principal
Disallow: /.git/             # Diretório Git (se exposto acidentalmente)
Disallow: /.aws/             # Diretório de configurações AWS
Disallow: /.ssh/             # Diretório de chaves SSH
Disallow: /phpinfo.php       # Arquivo que exibe informações do PHP
Disallow: /vendor/           # Diretório de dependências do Composer
Disallow: /db_backup/        # Pasta de backups do banco de dados
Disallow: /*.sql$             # Qualquer arquivo .sql (backups de DB)
Disallow: /*.zip$             # Arquivos compactados
Disallow: /*.tar.gz$          # Arquivos tar.gz
Disallow: /*.bak$             # Arquivos de backup
Disallow: /*.log$             # Arquivos de log
Disallow: /*.old$             # Arquivos antigos/obsoletos
Disallow: /*.json$            # Arquivos JSON (se não forem parte do conteúdo público)
Disallow: /*.txt$             # Arquivos de texto (se não forem conteúdo público)

# --- Arquivos Estáticos (CSS/JS/Imagens) ---
# RECOMENDADO: Permitir que os robôs acessem arquivos CSS e JS.
# O Google precisa renderizar a página como um usuário para entender o conteúdo e o layout.
# Não use Disallow para /assets/js/, /assets/css/, /assets/images/
# Se você tiver imagens sensíveis, coloque-as em um diretório bloqueado como /uploads/

# --- Bloquear URLs Dinâmicas com Parâmetros de Filtro/Sessão ---
# O objetivo é evitar indexação de conteúdo duplicado ou irrelevante,
# otimizando o orçamento de rastreamento (crawl budget).
# NOTA: O parâmetro 'lang' é geralmente permitido para suportar hreflang no sitemap.

# Parâmetros de Paginação e Ordenação
Disallow: /*?*page=* # Evita indexação de páginas paginadas (e.g., ?page=2)
Disallow: /*?*sort_by=* # Evita indexação de variações de ordenação (e.g., ?sort_by=price_low)
Disallow: /*?*order_by=* # Outro padrão comum para ordenação

# Parâmetros de Filtros de Busca
Disallow: /*?*category=* # Filtros de categoria
Disallow: /*?*gender=* # Filtros de gênero
Disallow: /*?*city_id=* # Filtros de cidade
Disallow: /*?*min_age=* # Filtros de idade mínima
Disallow: /*?*max_age=* # Filtros de idade máxima
Disallow: /*?*price_min=* # Filtros de preço mínimo
Disallow: /*?*price_max=* # Filtros de preço máximo
Disallow: /*?*distance=* # Filtros de distância
Disallow: /*?*ethnicity=* # Filtros de etnia
Disallow: /*?*body_type=* # Filtros de tipo físico
Disallow: /*?*rating=* # Filtros de avaliação/estrelas
Disallow: /*?*status=* # Filtros de status (e.g., ?status=active)

# Novos filtros adicionados ou mapeados (se aplicável ao seu site)
Disallow: /*?*type=* # Tipo de busca específica (e.g., ?type=providers)
Disallow: /*?*verified=* # Filtro "verificado apenas"
Disallow: /*?*available=* # Filtro "disponível agora"
Disallow: /*?*feature=* # Filtros baseados em funcionalidades/características
Disallow: /*?*location=* # Se a localização for um parâmetro de filtro dinâmico

# Parâmetros de sessão, teste ou depuração
Disallow: /*?*token=* # Tokens de acesso (e.g., para calculadora, Stripe)
Disallow: /*?*session_id=* # IDs de sessão
Disallow: /*?*debug=* # Parâmetros de depuração
Disallow: /*?*preview=* # Parâmetros de pré-visualização
Disallow: /*?*ref=* # Parâmetros de referência/afiliados (se não quiser indexar)
Disallow: /*?*utm_* # Parâmetros UTM de rastreamento de campanha (Google Analytics já os ignora, mas é bom para outros crawlers)

# Bloquear páginas de resultados de busca internas se não adicionarem valor único
Disallow: /searchresults/    # Exemplo: se você tiver uma página /searchresults/
Disallow: /search/?* # Bloqueia a página de busca com parâmetros

# --- Diretiva Crawl-delay (Opcional) ---
# Esta diretiva sugere um atraso entre as requisições para alguns rastreadores (não Googlebot).
# Pode ajudar a reduzir a carga no servidor de bots bem-comportados.
# Crawl-delay: 5             # Atraso de 5 segundos entre as requisições (ajuste conforme necessário)

# --- Sitemap ---
# Informa aos crawlers onde encontrar seu sitemap XML para facilitar a descoberta de páginas.
Sitemap: https://bacosearch.com/sitemap.php

# --- NOTA IMPORTANTE SOBRE NOINDEX ---
# O robots.txt é uma diretriz para RASTREAMENTO, não para INDEXAÇÃO.
# Se você deseja GARANTIR que uma página não apareça nos resultados de busca,
# use a meta tag <meta name="robots" content="noindex"> dentro do <head> da página,
# ou o cabeçalho HTTP X-Robots-Tag: noindex.
# Exemplo de meta tag: <meta name="robots" content="noindex, follow">
